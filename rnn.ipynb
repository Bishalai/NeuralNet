{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae66e771",
   "metadata": {},
   "source": [
    "Recurrent Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdded645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694e024",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3f7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    # Shift indices to avoid overflow\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796c777",
   "metadata": {},
   "source": [
    "Main RNN class:\n",
    "- uses BPTT using AdaGrad instead of SGD\n",
    "- target text generation using RNN\n",
    "- simple RNN model fully implemented in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, hidden_size, seq_length, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: Number of unique characters (input/output dimension)\n",
    "        hidden_size: Number of neurons in the hidden layer\n",
    "        seq_length: Number of time steps to unroll for backprop\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # initialization: simple random * 0.01 works for simple RNNs\n",
    "        \n",
    "        # W_xh: Input to Hidden\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        \n",
    "        # W_hh: Hidden to Hidden (Recurrent connection)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        \n",
    "        # W_hy: Hidden to Output\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1)) # Hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1))  # Output bias\n",
    "\n",
    "        # Memory for Adagrad optimization (makes convergence much faster/stable)\n",
    "        # m variables = memory variables used for the Adagrad update\n",
    "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        inputs: List of integers (indices of characters) for the sequence\n",
    "        h_prev: The hidden state from the previous time step (t-1)\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for future use in backprop\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev) # Store previous state at t=-1\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        # Loop through time steps\n",
    "        for t in range(len(inputs)):\n",
    "            # One-Hot Encode Input\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][inputs[t]] = 1 # puts 1 at the index of the input character for eg if input = 3, then xs[t] = [0,0,0,1,0,...]\n",
    "            \n",
    "            # Update Hidden State\n",
    "            # h_t = tanh(Wxh * x_t + Whh * h_{t-1} + bias)\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
    "            \n",
    "            # y_t = Why * h_t + bias\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            \n",
    "            #  Probabilities (Softmax)\n",
    "            ps[t] = softmax(ys[t])\n",
    "            \n",
    "        return xs, hs, ys, ps\n",
    "\n",
    "    def backward(self, inputs, targets, xs, hs, ps):\n",
    "        \"\"\"\n",
    "        Performs Backpropagation Through Time (BPTT)\n",
    "        \"\"\"\n",
    "        # Initialize gradients to zero\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        # Gradient of hidden state from the \"future\" (starts at 0)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        # Loop BACKWARDS through time\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            # Output Gradient (Cross Entropy + Softmax)\n",
    "            # dy = probs - 1 (at target index)\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1 \n",
    "            \n",
    "            # Gradient for W_hy and b_y\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # Gradient for Hidden State\n",
    "            # Comes from two sources:\n",
    "            # a) The output of this step (dy)\n",
    "            # b) The hidden state of the next step (dhnext)\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            \n",
    "            # Gradient through Tanh nonlinearity\n",
    "            # dtanh = (1 - h^2) * dh\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            \n",
    "            # Gradients for W_xh, W_hh, b_h\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            \n",
    "            # Pass gradient to the previous time step\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "            \n",
    "        # Clip gradients to mitigate exploding gradients problem\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def update_params(self, dWxh, dWhh, dWhy, dbh, dby):\n",
    "        \"\"\"\n",
    "        Updates weights using Adagrad (Adaptive Gradient).\n",
    "        Standard SGD is very unstable for RNNs; Adagrad is much better for convergence.\n",
    "        \"\"\"\n",
    "        for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                      [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    def sample(self, seed_ix, n):\n",
    "        \"\"\"\n",
    "        Generates a sequence of integers from the model.\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1)) # Reset hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1 # One-hot input\n",
    "        \n",
    "        ixes = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            # Forward pass (one step)\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = softmax(y)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            \n",
    "            # Prepare input for next step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "    \n",
    "        # returns the list character indices generated from RNN        \n",
    "        return ixes\n",
    "\n",
    "    def train(self, data, epochs=1000):\n",
    "        # Preprocessing (Char <-> Int)\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        char_to_ix = { ch:i for i,ch in enumerate(chars) } # return char for index\n",
    "        ix_to_char = { i:ch for i,ch in enumerate(chars) } # return index for char\n",
    "        \n",
    "        print(f\"Data size: {data_size}, Vocab size: {vocab_size}\")\n",
    "        \n",
    "        # Reset parameters for this specific vocab size\n",
    "        self.__init__(vocab_size, self.hidden_size, self.seq_length, self.lr)\n",
    "        \n",
    "        n, p = 0, 0 # Iteration counter, data pointer\n",
    "        h_prev = np.zeros((self.hidden_size, 1)) # Initial hidden state\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # Prepare inputs and targets\n",
    "            # If we are at end of text, reset pointer\n",
    "            if p + self.seq_length + 1 >= len(data): \n",
    "                h_prev = np.zeros((self.hidden_size, 1))\n",
    "                p = 0\n",
    "                \n",
    "            inputs = [char_to_ix[ch] for ch in data[p:p+self.seq_length]]\n",
    "            targets = [char_to_ix[ch] for ch in data[p+1:p+self.seq_length+1]]\n",
    "\n",
    "            # Forward\n",
    "            xs, hs, ys, ps = self.forward(inputs, h_prev)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = 0\n",
    "            for t in range(len(inputs)):\n",
    "                loss += -np.log(ps[t][targets[t], 0])\n",
    "            \n",
    "            # Backward\n",
    "            dWxh, dWhh, dWhy, dbh, dby = self.backward(inputs, targets, xs, hs, ps)\n",
    "            \n",
    "            # Update\n",
    "            self.update_params(dWxh, dWhh, dWhy, dbh, dby)\n",
    "            \n",
    "            # Update pointers\n",
    "            h_prev = hs[len(inputs)-1] # Carry over hidden state\n",
    "            p += self.seq_length\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f'Epoch {i}, Loss: {loss:.4f}')\n",
    "                # Sample text to see progress\n",
    "                sample_ix = self.sample(inputs[0], 150)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "                print(f\"----\\n{txt}\\n----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb717494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset: 11900\n",
      "Unique characters: 22\n",
      "\n",
      "--- Starting Training ---\n",
      "Target pattern: Hope is the thing with feathers that perches in the soul. And sings the tune without the words and never stops at all. \n",
      "\n",
      "Data size: 11900, Vocab size: 22\n",
      "Epoch 0, Loss: 77.2765\n",
      "----\n",
      "ucgw fwlncupt.ieAuHvuiovho.acHrggacw.HrcgpatfuAuAAtuhtcHHAt..Hf.oun tdHcHfw iifAndiueiHih.A.otuwigdrHtsHwApHHcwHAhAipAicrAdivferHA Hlfshoatfrfevucfd e\n",
      "----\n",
      "Epoch 1000, Loss: 1.1133\n",
      "----\n",
      " thang with feathers that perches in the wotun nd ithout the words and never stops at pll. oll. ings in al. ings without the tope without nnd is the t\n",
      "----\n",
      "Epoch 2000, Loss: 0.3242\n",
      "----\n",
      "pe at all. Hope is the thing with feathers th thope ws thercuns in the soul. And sings the tune without the words and never stops at all. Hope is the \n",
      "----\n",
      "Epoch 3000, Loss: 0.1763\n",
      "----\n",
      "thand in the sings the tune without the words and never stops at all. Hope is the thing with feathers that perches in the soul. And sings the tune wit\n",
      "----\n",
      "Epoch 4000, Loss: 0.1305\n",
      "----\n",
      "e that per st peathers that perches in the soul. And sings the tune without the words and never stops at all. Hope is the thing with feathers that per\n",
      "----\n",
      "Epoch 5000, Loss: 0.1084\n",
      "----\n",
      " the tune words and never stops at all. Hope is the thing with feathers that perches in the soul. And sings the tune without the words and never stops\n",
      "----\n",
      "Epoch 6000, Loss: 0.0787\n",
      "----\n",
      " in the s that perches in the soul. And sings the tune without the words and never stops at all. Hope is the thing with feathers that perches in the s\n",
      "----\n",
      "Epoch 7000, Loss: 0.0612\n",
      "----\n",
      "n thing with feathers that perches in the soul. And sings the tune without the words and never stops at all. Hope is the thing with feathers that perc\n",
      "----\n",
      "Epoch 8000, Loss: 0.0644\n",
      "----\n",
      "thand in the s tne words and never stops at all. Hope is the thing with feathers that perche sings the tune without the words and never stops at all. \n",
      "----\n",
      "Epoch 9000, Loss: 0.0569\n",
      "----\n",
      "e theth feathers that perches in the soul. And sings the tune without the words and never stops at all. Hope is the thing with feathers that perches i\n",
      "----\n",
      "\n",
      "--- Final Generation ---\n",
      "Hope is the thing with feathers that perches in the soul. And sings the tune without the words and never stops at all. Hope is the thing with feathers that perches in the soul. And sings the tune witho\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Data ----------------------\n",
    "# The first verse poem Hope is the thing with feathers by Emily Dickinson\n",
    "text_segment = \"Hope is the thing with feathers that perches in the soul. And sings the tune without the words and never stops at all. \"\n",
    "\n",
    "# We repeat the text to create a longer stream for the training loop\n",
    "# This simulates a dataset where this sentence structure is common.\n",
    "data = text_segment * 100 \n",
    "\n",
    "print(f\"Total characters in dataset: {len(data)}\")\n",
    "print(f\"Unique characters: {len(set(data))}\")\n",
    "\n",
    "# Configure and Train ----------------------------\n",
    "\n",
    "# Parameters:\n",
    "# hidden_size=64: Enough memory to store the sentence structure\n",
    "# seq_length=25: The RNN looks at 25 characters at a time to predict the 26th\n",
    "# learning_rate=0.1: Standard for character-level RNNs\n",
    "rnn = RNN(vocab_size=0, hidden_size=64, seq_length=25, learning_rate=0.1)\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "print(f\"Target pattern: {text_segment}\\n\")\n",
    "\n",
    "# We train for 10000 epochs to ensure it perfectly memorizes the phrase\n",
    "rnn.train(data, epochs=10000)\n",
    "\n",
    "# ==========================================\n",
    "# Final Verification (Generation)\n",
    "# ==========================================\n",
    "print(\"\\n--- Final Generation ---\")\n",
    "# Seed the model with the first character 'H' and ask it to generate 200 characters\n",
    "chars = list(set(data))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "\n",
    "seed_char = 'H'\n",
    "seed_idx = char_to_ix[seed_char]\n",
    "\n",
    "sample_ix = rnn.sample(seed_idx, n=200)\n",
    "generated_text = seed_char + ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
