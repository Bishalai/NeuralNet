{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e068fc",
   "metadata": {},
   "source": [
    "## Artificial Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795a867",
   "metadata": {},
   "source": [
    "- Fully connected neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "39717ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "4d23503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions and their derivatives\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z)) \n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def identity_derivative(z):\n",
    "    return np.ones_like(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main neural net class block\n",
    "\n",
    "class NeuralNet():\n",
    "    '''Fully Connected Neural Network:\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        # initialize the numpy array with weights \n",
    "        # expected class parameters: no of layers, no of neurons in each layer\n",
    "\n",
    "        #self.layers = [5, 6, 3, 1] # 5 inputs-> 6 neuron layers (hidden), 3 nueonrs layer (hidden), output 1\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.input_len = self.layers[0]\n",
    "\n",
    "        # store weights for each layer in a n x lm array, bias : 1x lm\n",
    "        # lm: no of neurons in the layer l, n: (weights for a neuron/ no of inputs) no of ouputs from previous layers\n",
    "        # first is input layer so dont need weights\n",
    "        self.weights = [np.random.randn(self.layers[i],self.layers[i+1]) for i in range(len(self.layers)-1)] \n",
    "        self.biases = [np.zeros((1,self.layers[i+1])) for i in range(len(self.layers)-1)]\n",
    "\n",
    "        print(\"Initialized weights and biases for layers:\", self.layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward pass that stores A and Z for use in backward pass\n",
    "        # prediction will predict for given input x which is a n1 x n2 array where n1 is no of obs and n2 is no of features\n",
    "        # n2 must be equal to input len of the NN\n",
    "        if x.shape[1] != self.input_len:\n",
    "            print(\"Input features not matching the input into the NN\")\n",
    "            return\n",
    "        A = x\n",
    "        self.A_cache = [x]       # store A[0]\n",
    "        self.Z_cache = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                A = relu(Z) ## relu activation for hidden layers\n",
    "            else:\n",
    "                A = identity(Z) ## identity activation for output layer\n",
    "                \n",
    "            # store intermediate Z and A for use in backpropagation\n",
    "            self.Z_cache.append(Z)\n",
    "            self.A_cache.append(A)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, x, y_true, lr):\n",
    "        ##Backpropagation \n",
    "\n",
    "        y_pred = self.forward(x)  # ensure forward pass has been done\n",
    "\n",
    "        L = len(self.weights)              # total layers excluding input\n",
    "        m = y_true.shape[0]                # batch size\n",
    "\n",
    "        # Gradients container\n",
    "        dW = [None] * L\n",
    "        dB = [None] * L\n",
    "        dZ = [None] * L\n",
    "\n",
    "        # ---------- LAST LAYER ----------\n",
    "        # dz[L] = dŷ * g’(z[L])\n",
    "        dA = (y_pred - y_true)             # assume MSE derivative: dA = y_pred - y_true\n",
    "        dZ[L-1] = dA * identity_derivative(self.Z_cache[L-1]) # L = length of weights but index starts from 0\n",
    "\n",
    "        # dw[L] = dz[L] * a[L−1] \n",
    "        dW[L-1] = np.matmul(self.A_cache[L-1].T, dZ[L-1]) / m  # m is no of obs in batch, must divide the dot profuct \n",
    "        ## here a_cache[L-1] is the activation from previous layer, a cache also consists of inputs in index=0.. so indexing may be confusing\n",
    "        dB[L-1] = np.sum(dZ[L-1], axis=0, keepdims=True) / m\n",
    "\n",
    "\n",
    "        # ---------- HIDDEN LAYERS ----------\n",
    "        for l in range(L-2, -1, -1):\n",
    "            # dz[l] = g’[l](z[l]) * Σ_j dz[l+1]_j * w[l+1]_ij\n",
    "            \n",
    "            dZ[l] = np.matmul(dZ[l+1], self.weights[l+1].T) * relu_derivative(self.Z_cache[l])\n",
    "\n",
    "            # print(f\"Layer {l}: dZ : {dZ[l]}\")\n",
    "\n",
    "            # dw[l] = dz[l] * a[l−1]\n",
    "            dW[l] = np.matmul(self.A_cache[l].T, dZ[l]) / m\n",
    "            dB[l] = np.sum(dZ[l], axis=0, keepdims=True) / m\n",
    "\n",
    "            # print(f\"Layer {l}: dW : {dW[l]}, dB: {dB[l]}\")\n",
    "\n",
    "        # ------------ Gradient Descent Update ------------\n",
    "        for l in range(L):\n",
    "            self.weights[l] -= lr * dW[l]\n",
    "            self.biases[l]  -= lr * dB[l]\n",
    "            # print(f\"Updated weights layer {l}:\", dW[l])\n",
    "            # print(f\"Updated biases layer {l}:\", dB[l])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, lr=0.001):\n",
    "        # train using back propagation\n",
    "        # train function: will do training of parameters, input x (features), y (actual ouputs), no of epochs, learning rate\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(X, Y, lr)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = self.forward(X)\n",
    "                loss = (np.mean((y_pred - Y)**2))**0.5  # RMSE\n",
    "                print(f\"Epoch {epoch}, RMSE Loss={loss:.6f}\")\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # predict function: will give predictions for input x\n",
    "        A = self.forward(x)\n",
    "        return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "aa0b9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 8, 5, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-10.22180344,   3.07018614],\n",
       "       [-16.42908615,  10.95216453],\n",
       "       [ -6.92357916,   1.62823078],\n",
       "       [-21.78494006,  16.24334578],\n",
       "       [-13.36519041,  14.26644254]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet(layers = [10, 8, 5, 2])\n",
    "\n",
    "xin = np.random.randn(5, 10)\n",
    "\n",
    "nn.predict(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "96aab53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "32a6d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (442, 10) (442, 1)\n"
     ]
    }
   ],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data       \n",
    "Y = data.target.reshape(-1, 1)  # convert to column vector\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "e290334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632753, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567042, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286131, -0.02593034]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "4738525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.])"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "9fb88014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (353, 10) (353, 1)\n",
      "Test: (89, 10) (89, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "8f7eb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 32, 16, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[10, 32, 16, 1])   # input = 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d508821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, RMSE Loss=170.741624\n",
      "Epoch 100, RMSE Loss=71.056155\n",
      "Epoch 200, RMSE Loss=63.145065\n",
      "Epoch 300, RMSE Loss=59.682072\n",
      "Epoch 400, RMSE Loss=57.651234\n",
      "Epoch 500, RMSE Loss=56.678947\n",
      "Epoch 600, RMSE Loss=55.050453\n",
      "Epoch 700, RMSE Loss=54.844745\n",
      "Epoch 800, RMSE Loss=54.136940\n",
      "Epoch 900, RMSE Loss=53.858539\n",
      "Epoch 1000, RMSE Loss=53.471358\n",
      "Epoch 1100, RMSE Loss=53.508910\n",
      "Epoch 1200, RMSE Loss=53.935069\n",
      "Epoch 1300, RMSE Loss=53.575660\n",
      "Epoch 1400, RMSE Loss=53.391469\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train, Y_train, epochs=1500, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "7fe37c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Test Loss=51.935928\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = nn.predict(X_test)\n",
    "test_loss = (np.mean((Y_test_pred - Y_test)**2))**0.5  # RMSE\n",
    "print(f\"RMSE Test Loss={test_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
