{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e068fc",
   "metadata": {},
   "source": [
    "## Artificial Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795a867",
   "metadata": {},
   "source": [
    "- Fully connected neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "39717ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "4d23503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions and their derivatives\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z)) \n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def identity_derivative(z):\n",
    "    return np.ones_like(z)\n",
    "\n",
    "def softmax(z):\n",
    "    # numerical stability: subtract max\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / np.sum(e_z)\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    s = softmax(z)\n",
    "    # Jacobian matrix: diag(s) - s * s^T\n",
    "    return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "9430d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main neural net class block\n",
    "\n",
    "class NeuralNet():\n",
    "    '''Fully Connected Neural Network:\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        # initialize the numpy array with weights \n",
    "        # expected class parameters: no of layers, no of neurons in each layer\n",
    "\n",
    "        #self.layers = [5, 6, 3, 1] # 5 inputs-> 6 neuron layers (hidden), 3 nueonrs layer (hidden), output 1\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.input_len = self.layers[0]\n",
    "\n",
    "        # store weights for each layer in a n x lm array, bias : 1x lm\n",
    "        # lm: no of neurons in the layer l, n: (weights for a neuron/ no of inputs) no of ouputs from previous layers\n",
    "        # first is input layer so dont need weights\n",
    "        self.weights = [np.random.randn(self.layers[i],self.layers[i+1]) for i in range(len(self.layers)-1)] \n",
    "        self.biases = [np.zeros((1,self.layers[i+1])) for i in range(len(self.layers)-1)]\n",
    "\n",
    "        print(\"Initialized weights and biases for layers:\", self.layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward pass that stores A and Z for use in backward pass\n",
    "        # prediction will predict for given input x which is a n1 x n2 array where n1 is no of obs and n2 is no of features\n",
    "        # n2 must be equal to input len of the NN\n",
    "        if x.shape[1] != self.input_len:\n",
    "            print(\"Input features not matching the input into the NN\")\n",
    "            return\n",
    "        A = x\n",
    "        self.A_cache = [x]       # store A[0]\n",
    "        self.Z_cache = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                A = relu(Z) ## relu activation for hidden layers\n",
    "            else:\n",
    "                A = identity(Z) ## identity activation for output layer\n",
    "                \n",
    "            # store intermediate Z and A for use in backpropagation\n",
    "            self.Z_cache.append(Z)\n",
    "            self.A_cache.append(A)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, x, y_true, lr):\n",
    "        ##Backpropagation \n",
    "\n",
    "        y_pred = self.forward(x)  # ensure forward pass has been done\n",
    "\n",
    "        L = len(self.weights)              # total layers excluding input\n",
    "        m = y_true.shape[0]                # batch size\n",
    "\n",
    "        # Gradients container\n",
    "        dW = [None] * L\n",
    "        dB = [None] * L\n",
    "        dZ = [None] * L\n",
    "\n",
    "        # ---------- LAST LAYER ----------\n",
    "        # dz[L] = dŷ * g’(z[L])\n",
    "        dA = (y_pred - y_true)             # assume MSE derivative: dA = y_pred - y_true\n",
    "        dZ[L-1] = dA * identity_derivative(self.Z_cache[L-1]) # L = length of weights but index starts from 0\n",
    "\n",
    "        # dw[L] = dz[L] * a[L−1] \n",
    "        dW[L-1] = np.matmul(self.A_cache[L-1].T, dZ[L-1]) / m  # m is no of obs in batch, must divide the dot profuct \n",
    "        ## here a_cache[L-1] is the activation from previous layer, a cache also consists of inputs in index=0.. so indexing may be confusing\n",
    "        dB[L-1] = np.sum(dZ[L-1], axis=0, keepdims=True) / m\n",
    "\n",
    "\n",
    "        # ---------- HIDDEN LAYERS ----------\n",
    "        for l in range(L-2, -1, -1):\n",
    "            # dz[l] = g’[l](z[l]) * Σ_j dz[l+1]_j * w[l+1]_ij\n",
    "            \n",
    "            dZ[l] = np.matmul(dZ[l+1], self.weights[l+1].T) * relu_derivative(self.Z_cache[l])\n",
    "\n",
    "            # print(f\"Layer {l}: dZ : {dZ[l]}\")\n",
    "\n",
    "            # dw[l] = dz[l] * a[l−1]\n",
    "            dW[l] = np.matmul(self.A_cache[l].T, dZ[l]) / m\n",
    "            dB[l] = np.sum(dZ[l], axis=0, keepdims=True) / m\n",
    "\n",
    "            # print(f\"Layer {l}: dW : {dW[l]}, dB: {dB[l]}\")\n",
    "\n",
    "        # ------------ Gradient Descent Update ------------\n",
    "        for l in range(L):\n",
    "            self.weights[l] -= lr * dW[l]\n",
    "            self.biases[l]  -= lr * dB[l]\n",
    "            # print(f\"Updated weights layer {l}:\", dW[l])\n",
    "            # print(f\"Updated biases layer {l}:\", dB[l])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, lr=0.01, batch_size=0, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "        batch_size:\n",
    "            0 -> full batch\n",
    "            1 -> stochastic\n",
    "            k < N -> mini batch\n",
    "        \"\"\"\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Full batch training --> treat whole dataset as one batch\n",
    "        if batch_size == 0 or batch_size >= N:\n",
    "            batch_size = N\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data at the start of each epoch\n",
    "            indices = np.random.permutation(N)\n",
    "            X_shuffled = X[indices]\n",
    "            Y_shuffled = Y[indices]\n",
    "\n",
    "            # Iterate over mini-batches\n",
    "            for start in range(0, N, batch_size):\n",
    "                end = start + batch_size\n",
    "\n",
    "                # Properly handle leftover batch (last small batch)\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "\n",
    "                # Call backward pass to update weights\n",
    "                self.backward(X_batch, Y_batch, lr)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "            \n",
    "            y_pred = self.forward(X)\n",
    "            loss = (np.mean((y_pred - Y)**2))**0.5  # RMSE\n",
    "            print(f\"Epoch {epoch}, RMSE Loss={loss:.6f}\")\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # predict function: will give predictions for input x\n",
    "        A = self.forward(x)\n",
    "        return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "aa0b9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 8, 5, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.70670593,   0.64098616],\n",
       "       [ -2.3349557 ,  -1.91482866],\n",
       "       [-12.96582124,   5.49498647],\n",
       "       [  5.28973398,   1.15409753],\n",
       "       [ -0.35204961,  -0.18871631]])"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet(layers = [10, 8, 5, 2])\n",
    "\n",
    "xin = np.random.randn(5, 10)\n",
    "\n",
    "nn.predict(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "96aab53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "32a6d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (442, 10) (442, 1)\n"
     ]
    }
   ],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data       \n",
    "Y = data.target.reshape(-1, 1)  # convert to column vector\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "e290334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632753, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567042, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286131, -0.02593034]])"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "4738525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.])"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "9fb88014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (353, 10) (353, 1)\n",
      "Test: (89, 10) (89, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "8f7eb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 32, 16, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[10, 32, 16, 1])   # input = 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "6d508821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed.\n",
      "Epoch 0, RMSE Loss=74.834424\n",
      "Epoch 2/10 completed.\n",
      "Epoch 1, RMSE Loss=81.898185\n",
      "Epoch 3/10 completed.\n",
      "Epoch 2, RMSE Loss=69.199672\n",
      "Epoch 4/10 completed.\n",
      "Epoch 3, RMSE Loss=73.588714\n",
      "Epoch 5/10 completed.\n",
      "Epoch 4, RMSE Loss=68.416735\n",
      "Epoch 6/10 completed.\n",
      "Epoch 5, RMSE Loss=72.477679\n",
      "Epoch 7/10 completed.\n",
      "Epoch 6, RMSE Loss=75.998826\n",
      "Epoch 8/10 completed.\n",
      "Epoch 7, RMSE Loss=65.096622\n",
      "Epoch 9/10 completed.\n",
      "Epoch 8, RMSE Loss=71.481783\n",
      "Epoch 10/10 completed.\n",
      "Epoch 9, RMSE Loss=60.642082\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train, Y_train, epochs=10, batch_size= 30, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "7fe37c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Test Loss=57.245478\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = nn.predict(X_test)\n",
    "test_loss = (np.mean((Y_test_pred - Y_test)**2))**0.5  # RMSE\n",
    "print(f\"RMSE Test Loss={test_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
