{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e068fc",
   "metadata": {},
   "source": [
    "## Artificial Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795a867",
   "metadata": {},
   "source": [
    "- Fully connected neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "39717ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "4d23503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions and their derivatives\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z)) \n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def identity_derivative(z):\n",
    "    return np.ones_like(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main neural net class block\n",
    "\n",
    "class NeuralNet():\n",
    "    '''Fully Connected Neural Network:\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        # initialize the numpy array with weights \n",
    "        # expected class parameters: no of layers, no of neurons in each layer\n",
    "\n",
    "        #self.layers = [5, 6, 3, 1] # 5 inputs-> 6 neuron layers (hidden), 3 nueonrs layer (hidden), output 1\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.input_len = self.layers[0]\n",
    "\n",
    "        # store weights for each layer in a n x lm array, bias : 1x lm\n",
    "        # lm: no of neurons in the layer l, n: (weights for a neuron/ no of inputs) no of ouputs from previous layers\n",
    "        # first is input layer so dont need weights\n",
    "        self.weights = [np.random.randn(self.layers[i],self.layers[i+1]) for i in range(len(self.layers)-1)] \n",
    "        self.biases = [np.zeros((1,self.layers[i+1])) for i in range(len(self.layers)-1)]\n",
    "\n",
    "        print(\"Initialized weights and biases for layers:\", self.layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward pass that stores A and Z for use in backward pass\n",
    "        # prediction will predict for given input x which is a n1 x n2 array where n1 is no of obs and n2 is no of features\n",
    "        # n2 must be equal to input len of the NN\n",
    "        if x.shape[1] != self.input_len:\n",
    "            print(\"Input features not matching the input into the NN\")\n",
    "            return\n",
    "        A = x\n",
    "        self.A_cache = [x]       # store A[0]\n",
    "        self.Z_cache = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                A = relu(Z) ## relu activation for hidden layers\n",
    "            else:\n",
    "                A = identity(Z) ## identity activation for output layer\n",
    "            # store intermediate Z and A for use in backpropagation\n",
    "            self.Z_cache.append(Z)\n",
    "            self.A_cache.append(A)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, x, y_true, lr):\n",
    "        ##Backpropagation \n",
    "\n",
    "        y_pred = self.forward(x)  # ensure forward pass has been done\n",
    "\n",
    "        L = len(self.weights)              # total layers excluding input\n",
    "        m = y_true.shape[0]                # batch size\n",
    "\n",
    "        # Gradients container\n",
    "        dW = [None] * L\n",
    "        dB = [None] * L\n",
    "        dZ = [None] * L\n",
    "\n",
    "        # ---------- LAST LAYER ----------\n",
    "        # dz[L] = dŷ * g’(z[L])\n",
    "        dA = (y_pred - y_true)             # assume MSE derivative: dA = y_pred - y_true\n",
    "        dZ[L-1] = dA * identity_derivative(self.Z_cache[L-1]) # L = length of weights but index starts from 0\n",
    "\n",
    "        # dw[L] = dz[L] * a[L−1] \n",
    "        dW[L-1] = np.matmul(self.A_cache[L-1].T, dZ[L-1]) / m  # m is no of obs in batch, must divide the dot profuct \n",
    "        ## here a_cache[L-1] is the activation from previous layer, a cache also consists of inputs in index=0.. so indexing may be confusing\n",
    "        dB[L-1] = np.sum(dZ[L-1], axis=0, keepdims=True) / m\n",
    "\n",
    "\n",
    "        # ---------- HIDDEN LAYERS ----------\n",
    "        for l in range(L-2, -1, -1):\n",
    "            # dz[l] = g’[l](z[l]) * Σ_j dz[l+1]_j * w[l+1]_ij\n",
    "            \n",
    "            dZ[l] = np.matmul(dZ[l+1], self.weights[l+1].T) * relu_derivative(self.Z_cache[l])\n",
    "\n",
    "            # print(f\"Layer {l}: dZ : {dZ[l]}\")\n",
    "\n",
    "            # dw[l] = dz[l] * a[l−1]\n",
    "            dW[l] = np.matmul(self.A_cache[l].T, dZ[l]) / m\n",
    "            dB[l] = np.sum(dZ[l], axis=0, keepdims=True) / m\n",
    "\n",
    "            # print(f\"Layer {l}: dW : {dW[l]}, dB: {dB[l]}\")\n",
    "\n",
    "        # ------------ Gradient Descent Update ------------\n",
    "        for l in range(L):\n",
    "            self.weights[l] -= lr * dW[l]\n",
    "            self.biases[l]  -= lr * dB[l]\n",
    "            # print(f\"Updated weights layer {l}:\", dW[l])\n",
    "            # print(f\"Updated biases layer {l}:\", dB[l])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, lr=0.001):\n",
    "        # train using back propagation\n",
    "        # train function: will do training of parameters, input x (features), y (actual ouputs), no of epochs, learning rate\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(X, Y, lr)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = self.forward(X)\n",
    "                loss = (np.mean((y_pred - Y)**2))**0.5  # RMSE\n",
    "                print(f\"Epoch {epoch}, RMSE Loss={loss:.6f}\")\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # predict function: will give predictions for input x\n",
    "        A = self.forward(x)\n",
    "        return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "aa0b9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 8, 5, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5.95671399, -10.35520634],\n",
       "       [ 10.55129928, -13.30340221],\n",
       "       [-11.93356644,   3.65934233],\n",
       "       [-18.60742097,  -1.92548912],\n",
       "       [ -3.01294174,  -1.8834433 ]])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet(layers = [10, 8, 5, 2])\n",
    "\n",
    "xin = np.random.randn(5, 10)\n",
    "\n",
    "nn.predict(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "96aab53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "32a6d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (442, 10) (442, 1)\n"
     ]
    }
   ],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data       \n",
    "Y = data.target.reshape(-1, 1)  # convert to column vector\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "e290334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632753, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567042, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286131, -0.02593034]])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "4738525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "9fb88014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (353, 10) (353, 1)\n",
      "Test: (89, 10) (89, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "8f7eb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights and biases for layers: [10, 32, 16, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[10, 32, 16, 1])   # input = 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "6d508821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, RMSE Loss=171.284835\n",
      "Epoch 10, RMSE Loss=76.902106\n",
      "Epoch 20, RMSE Loss=74.232961\n",
      "Epoch 30, RMSE Loss=71.507275\n",
      "Epoch 40, RMSE Loss=69.214399\n",
      "Epoch 50, RMSE Loss=66.818747\n",
      "Epoch 60, RMSE Loss=64.620350\n",
      "Epoch 70, RMSE Loss=62.657642\n",
      "Epoch 80, RMSE Loss=130.139153\n",
      "Epoch 90, RMSE Loss=63.853522\n",
      "Epoch 100, RMSE Loss=78.452829\n",
      "Epoch 110, RMSE Loss=66.613420\n",
      "Epoch 120, RMSE Loss=69.562127\n",
      "Epoch 130, RMSE Loss=68.598190\n",
      "Epoch 140, RMSE Loss=67.241611\n",
      "Epoch 150, RMSE Loss=67.826387\n",
      "Epoch 160, RMSE Loss=66.443108\n",
      "Epoch 170, RMSE Loss=66.448088\n",
      "Epoch 180, RMSE Loss=65.793237\n",
      "Epoch 190, RMSE Loss=65.207356\n",
      "Epoch 200, RMSE Loss=64.815206\n",
      "Epoch 210, RMSE Loss=64.326169\n",
      "Epoch 220, RMSE Loss=64.001289\n",
      "Epoch 230, RMSE Loss=63.553613\n",
      "Epoch 240, RMSE Loss=63.276766\n",
      "Epoch 250, RMSE Loss=62.871199\n",
      "Epoch 260, RMSE Loss=62.520816\n",
      "Epoch 270, RMSE Loss=62.190143\n",
      "Epoch 280, RMSE Loss=61.913476\n",
      "Epoch 290, RMSE Loss=61.586373\n",
      "Epoch 300, RMSE Loss=61.350232\n",
      "Epoch 310, RMSE Loss=61.100339\n",
      "Epoch 320, RMSE Loss=60.834208\n",
      "Epoch 330, RMSE Loss=60.645813\n",
      "Epoch 340, RMSE Loss=60.525489\n",
      "Epoch 350, RMSE Loss=60.178778\n",
      "Epoch 360, RMSE Loss=60.005279\n",
      "Epoch 370, RMSE Loss=59.831519\n",
      "Epoch 380, RMSE Loss=59.489536\n",
      "Epoch 390, RMSE Loss=59.320054\n",
      "Epoch 400, RMSE Loss=59.142710\n",
      "Epoch 410, RMSE Loss=58.930709\n",
      "Epoch 420, RMSE Loss=58.747497\n",
      "Epoch 430, RMSE Loss=58.603010\n",
      "Epoch 440, RMSE Loss=58.480646\n",
      "Epoch 450, RMSE Loss=58.315936\n",
      "Epoch 460, RMSE Loss=58.133177\n",
      "Epoch 470, RMSE Loss=58.005336\n",
      "Epoch 480, RMSE Loss=57.920279\n",
      "Epoch 490, RMSE Loss=57.744608\n",
      "Epoch 500, RMSE Loss=57.519120\n",
      "Epoch 510, RMSE Loss=57.415548\n",
      "Epoch 520, RMSE Loss=57.347108\n",
      "Epoch 530, RMSE Loss=57.224234\n",
      "Epoch 540, RMSE Loss=57.056187\n",
      "Epoch 550, RMSE Loss=56.906683\n",
      "Epoch 560, RMSE Loss=56.798957\n",
      "Epoch 570, RMSE Loss=56.712408\n",
      "Epoch 580, RMSE Loss=56.591919\n",
      "Epoch 590, RMSE Loss=56.461980\n",
      "Epoch 600, RMSE Loss=56.406486\n",
      "Epoch 610, RMSE Loss=56.323874\n",
      "Epoch 620, RMSE Loss=56.227942\n",
      "Epoch 630, RMSE Loss=56.159929\n",
      "Epoch 640, RMSE Loss=56.080606\n",
      "Epoch 650, RMSE Loss=56.057828\n",
      "Epoch 660, RMSE Loss=55.986770\n",
      "Epoch 670, RMSE Loss=55.930061\n",
      "Epoch 680, RMSE Loss=55.837324\n",
      "Epoch 690, RMSE Loss=55.764914\n",
      "Epoch 700, RMSE Loss=55.674039\n",
      "Epoch 710, RMSE Loss=55.588975\n",
      "Epoch 720, RMSE Loss=55.519563\n",
      "Epoch 730, RMSE Loss=55.480443\n",
      "Epoch 740, RMSE Loss=55.397031\n",
      "Epoch 750, RMSE Loss=55.332433\n",
      "Epoch 760, RMSE Loss=55.261860\n",
      "Epoch 770, RMSE Loss=55.243694\n",
      "Epoch 780, RMSE Loss=55.252630\n",
      "Epoch 790, RMSE Loss=55.203581\n",
      "Epoch 800, RMSE Loss=55.084856\n",
      "Epoch 810, RMSE Loss=55.016585\n",
      "Epoch 820, RMSE Loss=54.988358\n",
      "Epoch 830, RMSE Loss=54.961779\n",
      "Epoch 840, RMSE Loss=54.962822\n",
      "Epoch 850, RMSE Loss=54.967681\n",
      "Epoch 860, RMSE Loss=54.932759\n",
      "Epoch 870, RMSE Loss=54.863934\n",
      "Epoch 880, RMSE Loss=54.763149\n",
      "Epoch 890, RMSE Loss=54.612222\n",
      "Epoch 900, RMSE Loss=54.513965\n",
      "Epoch 910, RMSE Loss=54.505357\n",
      "Epoch 920, RMSE Loss=54.596706\n",
      "Epoch 930, RMSE Loss=54.741908\n",
      "Epoch 940, RMSE Loss=54.787881\n",
      "Epoch 950, RMSE Loss=54.768512\n",
      "Epoch 960, RMSE Loss=54.675209\n",
      "Epoch 970, RMSE Loss=54.517869\n",
      "Epoch 980, RMSE Loss=54.411500\n",
      "Epoch 990, RMSE Loss=54.375447\n",
      "Epoch 1000, RMSE Loss=54.355287\n",
      "Epoch 1010, RMSE Loss=54.387795\n",
      "Epoch 1020, RMSE Loss=54.406057\n",
      "Epoch 1030, RMSE Loss=54.400115\n",
      "Epoch 1040, RMSE Loss=54.363062\n",
      "Epoch 1050, RMSE Loss=54.348192\n",
      "Epoch 1060, RMSE Loss=54.355207\n",
      "Epoch 1070, RMSE Loss=54.343882\n",
      "Epoch 1080, RMSE Loss=54.326966\n",
      "Epoch 1090, RMSE Loss=54.379693\n",
      "Epoch 1100, RMSE Loss=54.414784\n",
      "Epoch 1110, RMSE Loss=54.392778\n",
      "Epoch 1120, RMSE Loss=54.343310\n",
      "Epoch 1130, RMSE Loss=54.291864\n",
      "Epoch 1140, RMSE Loss=54.243055\n",
      "Epoch 1150, RMSE Loss=54.246203\n",
      "Epoch 1160, RMSE Loss=54.240976\n",
      "Epoch 1170, RMSE Loss=54.214035\n",
      "Epoch 1180, RMSE Loss=54.259194\n",
      "Epoch 1190, RMSE Loss=54.220270\n",
      "Epoch 1200, RMSE Loss=54.172357\n",
      "Epoch 1210, RMSE Loss=54.135350\n",
      "Epoch 1220, RMSE Loss=54.095691\n",
      "Epoch 1230, RMSE Loss=54.090643\n",
      "Epoch 1240, RMSE Loss=54.083929\n",
      "Epoch 1250, RMSE Loss=54.078964\n",
      "Epoch 1260, RMSE Loss=54.069066\n",
      "Epoch 1270, RMSE Loss=54.078026\n",
      "Epoch 1280, RMSE Loss=54.106350\n",
      "Epoch 1290, RMSE Loss=54.092957\n",
      "Epoch 1300, RMSE Loss=54.052180\n",
      "Epoch 1310, RMSE Loss=54.086890\n",
      "Epoch 1320, RMSE Loss=54.125809\n",
      "Epoch 1330, RMSE Loss=54.123606\n",
      "Epoch 1340, RMSE Loss=54.114938\n",
      "Epoch 1350, RMSE Loss=54.075062\n",
      "Epoch 1360, RMSE Loss=54.049827\n",
      "Epoch 1370, RMSE Loss=54.035441\n",
      "Epoch 1380, RMSE Loss=54.019757\n",
      "Epoch 1390, RMSE Loss=54.026264\n",
      "Epoch 1400, RMSE Loss=54.039195\n",
      "Epoch 1410, RMSE Loss=54.012659\n",
      "Epoch 1420, RMSE Loss=53.987104\n",
      "Epoch 1430, RMSE Loss=54.085893\n",
      "Epoch 1440, RMSE Loss=54.077287\n",
      "Epoch 1450, RMSE Loss=54.022078\n",
      "Epoch 1460, RMSE Loss=54.003962\n",
      "Epoch 1470, RMSE Loss=54.089856\n",
      "Epoch 1480, RMSE Loss=54.045617\n",
      "Epoch 1490, RMSE Loss=54.064951\n",
      "Epoch 1500, RMSE Loss=54.023385\n",
      "Epoch 1510, RMSE Loss=54.077464\n",
      "Epoch 1520, RMSE Loss=54.056555\n",
      "Epoch 1530, RMSE Loss=54.016109\n",
      "Epoch 1540, RMSE Loss=53.985522\n",
      "Epoch 1550, RMSE Loss=53.970242\n",
      "Epoch 1560, RMSE Loss=53.965941\n",
      "Epoch 1570, RMSE Loss=53.970638\n",
      "Epoch 1580, RMSE Loss=53.992269\n",
      "Epoch 1590, RMSE Loss=54.087938\n",
      "Epoch 1600, RMSE Loss=54.148624\n",
      "Epoch 1610, RMSE Loss=54.109925\n",
      "Epoch 1620, RMSE Loss=53.984797\n",
      "Epoch 1630, RMSE Loss=53.899549\n",
      "Epoch 1640, RMSE Loss=53.890108\n",
      "Epoch 1650, RMSE Loss=53.926822\n",
      "Epoch 1660, RMSE Loss=53.964598\n",
      "Epoch 1670, RMSE Loss=53.927289\n",
      "Epoch 1680, RMSE Loss=53.917207\n",
      "Epoch 1690, RMSE Loss=54.000776\n",
      "Epoch 1700, RMSE Loss=53.993926\n",
      "Epoch 1710, RMSE Loss=53.992390\n",
      "Epoch 1720, RMSE Loss=54.015564\n",
      "Epoch 1730, RMSE Loss=54.005631\n",
      "Epoch 1740, RMSE Loss=53.972784\n",
      "Epoch 1750, RMSE Loss=54.018161\n",
      "Epoch 1760, RMSE Loss=53.931684\n",
      "Epoch 1770, RMSE Loss=53.885760\n",
      "Epoch 1780, RMSE Loss=53.962893\n",
      "Epoch 1790, RMSE Loss=54.032179\n",
      "Epoch 1800, RMSE Loss=54.129715\n",
      "Epoch 1810, RMSE Loss=54.144969\n",
      "Epoch 1820, RMSE Loss=54.073877\n",
      "Epoch 1830, RMSE Loss=54.001415\n",
      "Epoch 1840, RMSE Loss=53.786043\n",
      "Epoch 1850, RMSE Loss=53.640809\n",
      "Epoch 1860, RMSE Loss=53.610964\n",
      "Epoch 1870, RMSE Loss=53.699748\n",
      "Epoch 1880, RMSE Loss=53.847693\n",
      "Epoch 1890, RMSE Loss=54.006754\n",
      "Epoch 1900, RMSE Loss=54.207629\n",
      "Epoch 1910, RMSE Loss=54.203428\n",
      "Epoch 1920, RMSE Loss=54.104703\n",
      "Epoch 1930, RMSE Loss=53.947480\n",
      "Epoch 1940, RMSE Loss=53.739458\n",
      "Epoch 1950, RMSE Loss=53.670034\n",
      "Epoch 1960, RMSE Loss=53.694026\n",
      "Epoch 1970, RMSE Loss=53.794335\n",
      "Epoch 1980, RMSE Loss=53.876071\n",
      "Epoch 1990, RMSE Loss=53.981646\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train, Y_train, epochs=2000, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "7fe37c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Test Loss=52.360070\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = nn.predict(X_test)\n",
    "test_loss = (np.mean((Y_test_pred - Y_test)**2))**0.5  # RMSE\n",
    "print(f\"RMSE Test Loss={test_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
